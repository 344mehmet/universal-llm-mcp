# ğŸ–¥ï¸ LM Studio GeliÅŸtirici Rehberi

## TÃ¼rkÃ§e LM Studio Developer Guide - LLM'ler Ä°Ã§in

### LM Studio Nedir?

LM Studio, yerel LLM'leri Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±lan masaÃ¼stÃ¼ uygulamasÄ±dÄ±r. OpenAI-uyumlu REST API saÄŸlar.

---

## ğŸ”§ API Ã–zellikleri

### Temel Bilgiler

| Ã–zellik | DeÄŸer |
| ------- | ----- |
| API URL | `http://localhost:1234` |
| API Stili | OpenAI-uyumlu |
| Model YÃ¼kleme | JIT (Just-in-Time) |
| Formatlar | GGUF quantized |

### Desteklenen Endpoint'ler

```text
GET  /v1/models              - Model listesi
POST /v1/chat/completions    - Chat tamamlama
POST /v1/completions         - Metin tamamlama
POST /v1/embeddings          - Embedding oluÅŸturma
```

---

## ğŸ’» JavaScript/TypeScript KullanÄ±mÄ±

### Temel Ä°stek

```typescript
const LMSTUDIO_URL = "http://localhost:1234";

async function chatTamamla(mesaj: string): Promise<string> {
    const yanit = await fetch(`${LMSTUDIO_URL}/v1/chat/completions`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
            model: "auto",  // otomatik seÃ§im
            messages: [
                { role: "system", content: "Sen yardÄ±msever bir asistandsÄ±n." },
                { role: "user", content: mesaj }
            ],
            temperature: 0.7,
            max_tokens: 2048
        })
    });
    
    const veri = await yanit.json();
    return veri.choices[0].message.content;
}
```

### Streaming YanÄ±t

```typescript
async function* streamChat(mesaj: string): AsyncGenerator<string> {
    const yanit = await fetch(`${LMSTUDIO_URL}/v1/chat/completions`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
            model: "auto",
            messages: [{ role: "user", content: mesaj }],
            stream: true
        })
    });
    
    const reader = yanit.body?.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
        const { done, value } = await reader!.read();
        if (done) break;
        
        const satir = decoder.decode(value);
        if (satir.startsWith("data: ")) {
            const json = JSON.parse(satir.slice(6));
            const icerik = json.choices[0]?.delta?.content;
            if (icerik) yield icerik;
        }
    }
}
```

---

## ğŸ Python KullanÄ±mÄ±

### OpenAI SDK ile

```python
from openai import OpenAI

istemci = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio"  # herhangi bir deÄŸer olabilir
)

yanit = istemci.chat.completions.create(
    model="auto",
    messages=[
        {"role": "system", "content": "Sen yardÄ±msever bir asistandsÄ±n."},
        {"role": "user", "content": "Merhaba, nasÄ±lsÄ±n?"}
    ],
    temperature=0.7
)

print(yanit.choices[0].message.content)
```

### LM Studio SDK ile

```python
# pip install lmstudio-python
from lmstudio import Client

istemci = Client()

# Model yÃ¼kle
model = istemci.llm.load("deepseek/deepseek-r1-0528-qwen3-8b")

# Chat oluÅŸtur
yanit = model.respond("Python'da liste nasÄ±l oluÅŸturulur?")
print(yanit)
```

---

## ğŸ”¨ CLI AraÃ§larÄ±

LM Studio CLI (`lms`) komutu ile terminal Ã¼zerinden kontrol:

```bash
# LM Studio durumunu kontrol et
lms status

# Model listele
lms ls

# Model yÃ¼kle
lms load deepseek/deepseek-r1-0528-qwen3-8b

# Model indir
lms get qwen/qwq-32b

# Sunucuyu baÅŸlat
lms server start
```

---

## ğŸ§° Function Calling (Tool Use)

LM Studio, OpenAI-uyumlu tool calling destekler:

```typescript
const araÃ§lar = [
    {
        type: "function",
        function: {
            name: "havadurumu_al",
            description: "Belirtilen ÅŸehrin hava durumunu al",
            parameters: {
                type: "object",
                properties: {
                    sehir: { 
                        type: "string", 
                        description: "Åehir adÄ±" 
                    }
                },
                required: ["sehir"]
            }
        }
    }
];

const yanit = await fetch(`${LMSTUDIO_URL}/v1/chat/completions`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
        model: "auto",
        messages: [
            { role: "user", content: "Ä°stanbul'da hava nasÄ±l?" }
        ],
        tools: araÃ§lar,
        tool_choice: "auto"
    })
});
```

---

## ğŸ“Š Performans Ä°puÃ§larÄ±

### GPU Offload

- **Tam GPU:** TÃ¼m katmanlarÄ± GPU'ya yÃ¼kle
- **KÄ±smi GPU:** Bellek sÄ±nÄ±rlÄ±ysa bazÄ± katmanlarÄ± RAM'de tut
- **Sadece CPU:** GPU yoksa veya kÃ¼Ã§Ã¼k modeller iÃ§in

### Context Length

- VarsayÄ±lan: 4096 token
- Maksimum: Model baÄŸÄ±mlÄ± (bazÄ±larÄ± 128K destekler)
- ArttÄ±kÃ§a bellek kullanÄ±mÄ± artar

### Ã–nerilen Ayarlar (16GB VRAM)

| Model | GPU Layers | Context |
| ----- | ---------- | ------- |
| 7B-Q4 | 35 | 8192 |
| 14B-Q4 | 28 | 4096 |
| 32B-Q4 | 20 | 4096 |

---

## ğŸ”Œ Entegrasyon Ã–rnekleri

### VSCode Continue ile

```json
{
    "models": [{
        "title": "LM Studio",
        "provider": "lmstudio",
        "model": "auto"
    }]
}
```

### LangChain ile

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:1234/v1",
    api_key="lm-studio",
    model="auto"
)

yanit = llm.invoke("Python'da decorator nedir?")
print(yanit.content)
```

---
**LLM Notu:** Bu dokÃ¼man LM Studio geliÅŸtirici Ã¶zellikleri iÃ§in TÃ¼rkÃ§e RAG kaynaÄŸÄ±dÄ±r.
